{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN/sKiBN0meATFsLCE/XsYV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nikhil-singh955/AI-Project/blob/main/PySpark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. What is PySpark\n",
        "\n",
        "Ans1. PySpark is the Python API for Apache Spark. It enables Python developers to harness the power of Spark's distributed computing capabilities for big data processing and analytics while using Python, one of the most popular programming languages. PySpark provides easy access to Spark's core functionalities, including data processing, machine learning, and real-time analytics.\n"
      ],
      "metadata": {
        "id": "K4MrdNJyCZJK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. What are the industrial benefits of PySpark?\n",
        "\n",
        "These days, almost every industry makes use of big data to evaluate where they stand and grow. When you hear the term big data, Apache Spark comes to mind. Following are the industry benefits of using PySpark that supports Spark:\n",
        "\n",
        "# Media streaming:\n",
        "Spark can be used to achieve real-time streaming to provide personalized recommendations to subscribers. Netflix is one such example that uses Apache Spark. It processes around 450 billion events every day to flow to its server-side apps.\n",
        "\n",
        "# Finance:\n",
        "Banks use Spark for accessing and analyzing the social media profiles and in turn get insights on what strategies would help them to make the right decisions regarding customer segmentation, credit risk assessments, early fraud detection etc.\n",
        "\n",
        "# Healthcare:\n",
        "Providers use Spark for analyzing the past records of the patients to identify what health issues the patients might face posting their discharge. Spark is also used to perform genome sequencing for reducing the time required for processing genome data.\n",
        "\n",
        "# Travel Industry:\n",
        "Companies like TripAdvisor uses Spark to help users plan the perfect trip and provide personalized recommendations to the travel enthusiasts by comparing data and review from hundreds of websites regarding the place, hotels, etc.\n",
        "\n",
        "# Retail and e-commerce:\n",
        "This is one important industry domain that requires big data analysis for targeted advertising. Companies like Alibaba run Spark jobs for analyzing petabytes of data for enhancing customer experience, providing targetted offers, sales and optimizing the overall performance."
      ],
      "metadata": {
        "id": "Kpq_DYkDCo1E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3 What is PySpark UDF?\n",
        "\n",
        "UDF stands for User Defined Functions. In PySpark, UDF can be created by creating a python function and wrapping it with PySpark SQL’s udf() method and using it on the DataFrame or SQL. These are generally created when we do not have the functionalities supported in PySpark’s library and we have to use our own logic on the data. UDFs can be reused on any number of SQL expressions or DataFrames."
      ],
      "metadata": {
        "id": "oStvbZg4FmcH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. What is SparkSession in Pyspark?\n",
        "\n",
        "SparkSession is the entry point to PySpark and is the replacement of SparkContext since PySpark version 2.0. This acts as a starting point to access all of the PySpark functionalities related to RDDs, DataFrame, Datasets etc. It is also a Unified API that is used in replacing the SQLContext, StreamingContext, HiveContext and all other contexts"
      ],
      "metadata": {
        "id": "DnnKhbNEF85O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. What do you understand about PySpark DataFrames?\n",
        "\n",
        "PySpark DataFrame is a distributed collection of well-organized data that is equivalent to tables of the relational databases and are placed into named columns. PySpark DataFrame has better optimisation when compared to R or python. These can be created from different sources like Hive Tables, Structured Data Files, existing RDDs, external databases etc as shown in the image below:"
      ],
      "metadata": {
        "id": "JQTTDO5hGUi2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. Is PySpark faster than pandas?\n",
        "\n",
        "PySpark supports parallel execution of statements in a distributed environment, i.e on different cores and different machines which are not present in Pandas. This is why PySpark is faster than pandas."
      ],
      "metadata": {
        "id": "gcWgJcvMIS4F"
      }
    }
  ]
}